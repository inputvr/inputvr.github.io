<!DOCTYPE html>
<!-- saved from url=(0028)https://inputVR.github.io/ -->
<html lang="en"><head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<meta property="og:image" content="index_files/urlIcon.png"/>
	<link rel="stylesheet" href="index_files/uikit.min.css">
	<link href="index_files/style.css" rel="stylesheet">
   <style>undefined</style><link rel="preconnect" href="https://fonts.googleapis.com/" crossorigin="true"><link rel="preconnect" href="https://fonts.gstatic.com/"><link rel="stylesheet" href="index_files//css2"></head>

   <body data-new-gr-c-s-check-loaded="14.1086.0" data-gr-ext-installed="">
    <header>
      <div id="logo-area" uk-grid="" class="uk-grid">
  		 <div class="uk-width-3-5 uk-first-column">

     		 <div><a href="https://inputVR.github.io/index.html" class="uk-logo"> Input for VR/AR workshop</a></div>
        <div id="citation">IEEE VR 2024 Workshop - Input for Virtual Reality and Augmented Reality (IXR)<br>  <br></div>
     </div>
     
    </div></header>

   
   	<div id="page-container" class="full-viewport uk-grid-row-large uk-grid-collapse uk-grid" uk-grid="">
      <div class="leftcol uk-width-1-5 uk-first-column"></div>
      <div class="middlecol uk-width-4-5">
        <!-- <div id='workshop-title-div'> -->

         <div class="workshop-title">IEEE VR 2024 Workshop - Input for Virtual Reality and Augmented Reality (IXR)</div>
<!--         </div> -->


        <div class="workshop-subsection">Input for Virtual Reality and Augmented Reality (IXR) Workshop Overview</div>
        <p>Some decades ago experiments the use of VR required of participants and of experimenters. Wiht on click experimenters would put participants inside wonderful experiences. 
          But with the arrival of personal VR/AR devices. We also have seen the need to navigate UIs, and buttons and ultimatelly the need for participants to interact with the device and input their commands to it, without the help of any experimenter on the side.
          I.e. they find themselves trying to click on buttons or interacting with panels, and often failing at it, because their body isn't porperly remapped, there are missing sensors, or because they need to interact with far away content with high accuracy. Or for many other reasons.
          This workshop is for all Immersive Computing researchers and developers who have been searching for ways to solve this input problem. This is your venue, whether you look at this problem from a perspective of the natural interface, with implicit or explicit inputs, 
          whether it focus on the UI improvements needed for a particular input setup, whether it tries to create multimodal input, multidevice, or simply strives for backward compatibility with existing PC inputs. Or you are trying to find new sensing techniques or delivering haptic for 
          feedback on the confirmation of an input.

		</p>

         <div class="workshop-subsection">Description</div>
         <p>
		Driven by uptake in consumer and professional markets, virtual reality technologies are now being adopted at a significant pace. Users are however left on their own at home having to navigate the UIs to open their games or applications.
    While for a long time it seemed that controllers would be the way to interact, then this view got replaced by the minority report visions relying hevily in a hand input system. 
    However, there is mounting evidence that better approaches can exist as we add more sensors, that can play a multimodal role and techniques to transition across the vocabulary of interactions.		 
		</p>

		<p> 
		We invite all researchers to join the first edition of this workshop to contribute and learn from existing and novel input techniques. 
    We also invite submissions on technical and systems papers that describe new techniques, sensing or UIs, in which they explore ways to transition between all them.
     In this workshop, we will explore these input scenarios, their accessibility, and what type of applications they enable, their performance. Finally, we invite participants to also submit their position papers and share their learnings while using these inputs and interactions. 
     All papers will be collected and curated in the workshop website <a href="https://inputVR.github.io">  https://inputVR.github.io</a>
		</p>


    <div id="organizers">
              <div class="workshop-subsection">Workshop organizers</div>
             <div class="organizers uk-grid-row-medium uk-grid" uk-grid="">

              <div class="organizers uk-grid-row-medium uk-grid" uk-grid="">
            <div class="leftcol uk-width-1-4 uk-first-column"><img width="200" src="https://margonzalezfranco.github.io/MarGonzalez_files/me.jpg"></div>
            <div class="middlecol uk-width-3-4">
              <h5>Mar Gonzalez-Franco, Google AR & VR, USA<br/> https://margonzalezfranco.github.io/ @twi_mar</h5>
              <p>Dr. Mar Gonzalez-Franco leads the BIRD - Blended Interaction Research and Devices - team at Google. She explores human behaviour and perception to build better technologies in the wild. With a particular focus on spatial computing, avatars, input and haptics. 
              A part from her scientific output,  her work has transferred to products used daily around the world, like Avatars on Teams, features on Hololens and MRTK, Microsoft Soundscape and Together mode in Microsoft Teams. 
              Mar is a Computer Scientist and also holds a PhD in Immersive Virtual Reality and Clinical Psychology from Universitat de Barcelona. Before joining Google, she was at Microsoft Resaerch for 7 years, and at Airbus, and priorly did academic research at MIT, University College London, and Tsinghua. </p>
            </div>			  
        
               			  			  
           		
           <div class="leftcol uk-width-1-4 uk-grid-margin uk-first-column"><img width="200" src="https://pure.au.dk/portal/files/222876236/AU_photo.jpg"></div>
               <div class="middlecol uk-width-3-4 uk-grid-margin">
                 <h5>Ken Pfeufer, Aarhus University, Denmark <br>https://kenpfeuffer.com/</h5>
                 <p>Is a tenure-track Assistant Professor in the Ubiquitous Computing and Interaction group at the computer science department at Aarhus University, Denmark. 
                  Where he leads the XI research group on topics of human-computer interaction, in particular AR/VR/XR, eye & hand interaction, UI design, and adaptive UI. 
                  Before, he was at Florian Alt‘s Usable Security and Privacy group in Munich, at Lancaster University in the UK at Hans Gellersen‘s group, and interned at Microsoft and Google Research.</p>
               </div>

	<div class="leftcol uk-width-1-4 uk-grid-margin uk-first-column"><img width="200" src="https://jaeyeonlee.com/img/JL0823_0a_sq.jpg"></div>
               <div class="middlecol uk-width-3-4 uk-grid-margin">
                 <h5>Jaeyeon Lee, UNIST, Republic of Korea <br>https://jaeyeonlee.com/</h5>
                 <p>Is an Assistant Professor in Computer Science and Engineering at UNIST (Ulsan National Institute of Science and Technology), where she is leading the TACT Lab. Her research in Human-Computer Interaction focuses on designing rich and intuitive haptic interactions for future computers based on human haptic perception and innovative hardware interfaces. Previously, she earned her Ph.D. in Computer Science and her Master’s in Electrical Engineering from KAIST. She was also a research intern at Microsoft Research and Saarland University.</p>
               </div>
		      
                		
           <div class="leftcol uk-width-1-4 uk-grid-margin uk-first-column"><img width="200" src="https://hemantsurale.com/assets/img/prof_pic.jpg"></div>
               <div class="middlecol uk-width-3-4 uk-grid-margin">
                 <h5>Hemant Surale, Meta, USA <br>https://hemantsurale.com/</h5>
                 <p>Hemant (meaning) is a Human-Computer Interaction Researcher in Canada. His main research interests include Mixed Reality Interfaces (MR), Wearable Computing, Input Interactions (Gestural interfaces), and Applied Machine Learning (Computer Vision).

He earned his Ph.D. in Computer Science from the University of Waterloo, Canada.  He has previously worked and interned with Microsoft Research, Snap Research, North Research, NetApp Research, Oracle Corp, and co-founded a startup, IoTBLR (Internet of Things, Bangalore).

The focus of his HCI research is on improving productivity through the design, development, and evaluation of user interfaces. He has studied atomic concepts such as modes and mode-switching to create design guidelines that can be applied to a wide range of applications.</p>
               </div>

		      
	<div class="leftcol uk-width-1-4 uk-grid-margin uk-first-column"><img width="200" src="https://seongkookheo.com/images/me.jpg"></div>
               <div class="middlecol uk-width-3-4 uk-grid-margin">
                 <h5>Seongkook Heo, University of Virginia, USA <br>https://seongkookheo.com/</h5>
                 <p>Is the Alfred Weaver Assistant Professor in the Department of Computer Science at the University of Virginia. He directs the Ultimate User Interface Lab, where they develop sensing and haptic feedback technologies and build interactive systems.
	          He was previously a postdocotral researcher in the DGP lab at the University of Toronto, where he worked with Prof. Daniel Wigdor.
			 Seongkook received his Ph.D. in Computer Science at  KAIST in 2017, advised by Prof. Geehyuk Lee. He was also a research intern at industry research labs, including Samsung Advanced Institute of Technology, Microsoft Research (with Dr. Ken Hinckley), and Autodesk Research (with Dr. Tovi Grossman).</p>
               </div>


		      	<div class="leftcol uk-width-1-4 uk-grid-margin uk-first-column"><img width="200" src="https://www.concordia.ca/etc/designs/concordia/resources/file.jpg?did=6480&w=303"></div>
               <div class="middlecol uk-width-3-4 uk-grid-margin">
                 <h5> Anil Ufuk Batmaz ,  Concordia University, Canada <br>https://users.encs.concordia.ca/~abatmaz/</h5>
                 <p> Is an assistant professor at Concordia University, Gina Cody School on Engineering and Computer Science, Computer Science and Software Engineering Department. Leading the EXtended Reality and Interaction Technologies (EXIT) Lab, his research focuses on Virtual Reality, Augmented Reality, 3D user interfaces, training systems, and simulators. 
			 Prior to his current role, he served as an assistant professor at Kadir Has University and completed a post-doctoral fellowship at Simon Fraser University. Dr. Batmaz earned his PhD in Biomedical Engineering from Strasbourg University.</p>
               </div>
        	  
               <br><br>
           
             </div>
   
             <br><br>  <br><br>

	<div id="callforpapers">

		  <div class="workshop-subsection"> CALL FOR PAPERS Workshop topics</div>
          <ul>
            <li>Input techniques in VR/AR</li>
            <li>Multimodal Input</li>
            <li>UI interaction</li>
            <li>Multidevice Input</li>
            <li>Sensing tools</li>  
            <li>Theory work</li> 
        </ul>
         <div class="workshop-subsection">Format and submission guidelines</div>
         <p>Submissions should include a title, a list of authors, and be 2-4 pages. All paper submissions must be in English. All IEEE VR Conference Paper submissions must be prepared in IEEE Computer Society VGTC format (<a href="https://tc.computer.org/vgtc/publications/conference/"> https://tc.computer.org/vgtc/publications/conference/</a>) and submitted in PDF format. 
           Accepting work of Research papers, Technical notes, Position papers, Work-in-progress papers.&nbsp;</p>
         <p>The accepted papers will be featured in the IEEE Xplore library. Additional pages can be considered on a case by case basis, but you should check with the workshop organizers (<a href="mailto:m.volonte@northeastern.edu">m.volonte@northeastern.edu.</a>) before the submission deadline. Acceptable paper types are work-in-progress, research papers, position papers, or commentaries. Submissions will be reviewed by the organisers and accepted submissions will give a 10-minute talk with a panel discussion at the end of the session. At least one author must register for the workshop. Selected submissions will get the opportunity to be extended to articles to be considered for a special issue.</p>
		<p>To submit your work, visit <a href="https://new.precisionconference.com/vr">https://new.precisionconference.com/vr</a></p>
		The organizers will review all the submissions.

        <div class="workshop-subsection">Important dates - Not available yet</div>
        <ul>
          <li>Submission deadline:&nbsp;</li>
          <li>Notification of acceptance:&nbsp; </li>
          <li>Camera-ready deadline:&nbsp; </li>
		  <li>Expected workshop date: </li>	

          <li>The workshop will be in person at IEEE VR 2024 in Orlando Florida USA</li>
		</div>
		      <br/><br/><br/><br/><br/><br/>
        <div class="workshop-subsection">Workshop agenda </div>
		      <br><br>
        <ul>
          <li>14:00-0:15: Introduction by the organizers </li>
     <li>14:15-14:30: Keynote-  </li>
          <li>14:30-16:00: Short presentations of the accepted papers: 10 min per submission + discussion</li>
		  <li>16:00-16:15: Discussion in the breakout rooms</li>	
          <li>16:15-16:30: Closing and call for improving Input in VR/AR</li>
		 </ul>



   
         


<br>
    <div id="footer-area" class="uk-grid-collapse uk-grid uk-grid-margin uk-first-column" uk-grid="">
          <div class="leftcol uk-width-1-5 uk-first-column"></div>
          <div class="middlecol uk-width-3-5">For questions and comments, please contact <a href="mailto:m.volonte@northeastern.edu">Matias Volonte</a>.</div>
          <div class="rightcol uk-width-1-5"></div>
    </div>

   

</div>
</div></div></body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>
